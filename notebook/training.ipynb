{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87d7721b",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7788d506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP Libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sklearn Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a4c5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download additional NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "print(\"✅ NLTK punkt_tab downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88239e63",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f6913e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (105, 2)\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Congratulations! You've won a $1000 gift card....</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey, are we still meeting for lunch tomorrow?</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>URGENT: Your account has been compromised. Ver...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can you send me the project files?</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  Congratulations! You've won a $1000 gift card....  spam\n",
       "1      Hey, are we still meeting for lunch tomorrow?   ham\n",
       "2  URGENT: Your account has been compromised. Ver...  spam\n",
       "3                 Can you send me the project files?   ham\n",
       "4  Free entry in 2 a wkly comp to win FA Cup fina...  spam"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../data/spam.csv')\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55bdb262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 105 entries, 0 to 104\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    105 non-null    object\n",
      " 1   label   105 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.8+ KB\n",
      "\n",
      "Missing values:\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "ham     53\n",
      "spam    52\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check dataset info\n",
    "print(\"Dataset Info:\")\n",
    "df.info()\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebf0ce89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize label distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "df['label'].value_counts().plot(kind='bar', color=['green', 'red'])\n",
    "plt.title('Distribusi Label Spam vs Ham', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Label', fontsize=12)\n",
    "plt.ylabel('Jumlah', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13fed87",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6528754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hello! This is a TEST message with punctuation.\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Miftah/nltk_data'\n    - 'd:\\\\Project Final Machine Learning\\\\spam_chat_detector_using_ML\\\\.venv\\\\nltk_data'\n    - 'd:\\\\Project Final Machine Learning\\\\spam_chat_detector_using_ML\\\\.venv\\\\share\\\\nltk_data'\n    - 'd:\\\\Project Final Machine Learning\\\\spam_chat_detector_using_ML\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Miftah\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m sample_text = \u001b[33m\"\u001b[39m\u001b[33mHello! This is a TEST message with punctuation.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOriginal:\u001b[39m\u001b[33m\"\u001b[39m, sample_text)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPreprocessed:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_text\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mpreprocess_text\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     12\u001b[39m text = text.translate(\u001b[38;5;28mstr\u001b[39m.maketrans(\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, string.punctuation))\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Tokenization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m tokens = \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Remove stopwords\u001b[39;00m\n\u001b[32m     18\u001b[39m stop_words = \u001b[38;5;28mset\u001b[39m(stopwords.words(\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project Final Machine Learning\\spam_chat_detector_using_ML\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project Final Machine Learning\\spam_chat_detector_using_ML\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project Final Machine Learning\\spam_chat_detector_using_ML\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project Final Machine Learning\\spam_chat_detector_using_ML\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project Final Machine Learning\\spam_chat_detector_using_ML\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project Final Machine Learning\\spam_chat_detector_using_ML\\.venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Miftah/nltk_data'\n    - 'd:\\\\Project Final Machine Learning\\\\spam_chat_detector_using_ML\\\\.venv\\\\nltk_data'\n    - 'd:\\\\Project Final Machine Learning\\\\spam_chat_detector_using_ML\\\\.venv\\\\share\\\\nltk_data'\n    - 'd:\\\\Project Final Machine Learning\\\\spam_chat_detector_using_ML\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Miftah\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Fungsi untuk preprocessing teks:\n",
    "    1. Lowercase\n",
    "    2. Remove punctuation\n",
    "    3. Remove stopwords\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Join tokens back to string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Test preprocessing function\n",
    "sample_text = \"Hello! This is a TEST message with punctuation.\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Preprocessed:\", preprocess_text(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1024342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing semua teks...\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Miftah/nltk_data'\n    - 'd:\\\\Project Final Machine Learning\\\\spam_chat_detector_using_ML\\\\.venv\\\\nltk_data'\n    - 'd:\\\\Project Final Machine Learning\\\\spam_chat_detector_using_ML\\\\.venv\\\\share\\\\nltk_data'\n    - 'd:\\\\Project Final Machine Learning\\\\spam_chat_detector_using_ML\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Miftah\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Apply preprocessing to all texts\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPreprocessing semua teks...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mcleaned_text\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ Preprocessing selesai!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mContoh hasil preprocessing:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project Final Machine Learning\\spam_chat_detector_using_ML\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:4943\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4808\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4809\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4810\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4815\u001b[39m     **kwargs,\n\u001b[32m   4816\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4817\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4818\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4819\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4934\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4935\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4936\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4937\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4939\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4941\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project Final Machine Learning\\spam_chat_detector_using_ML\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project Final Machine Learning\\spam_chat_detector_using_ML\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project Final Machine Learning\\spam_chat_detector_using_ML\\.venv\\Lib\\site-packages\\pandas\\core\\base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project Final Machine Learning\\spam_chat_detector_using_ML\\.venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mpreprocess_text\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     12\u001b[39m text = text.translate(\u001b[38;5;28mstr\u001b[39m.maketrans(\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, string.punctuation))\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Tokenization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m tokens = \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Remove stopwords\u001b[39;00m\n\u001b[32m     18\u001b[39m stop_words = \u001b[38;5;28mset\u001b[39m(stopwords.words(\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project Final Machine Learning\\spam_chat_detector_using_ML\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project Final Machine Learning\\spam_chat_detector_using_ML\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project Final Machine Learning\\spam_chat_detector_using_ML\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project Final Machine Learning\\spam_chat_detector_using_ML\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project Final Machine Learning\\spam_chat_detector_using_ML\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project Final Machine Learning\\spam_chat_detector_using_ML\\.venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Miftah/nltk_data'\n    - 'd:\\\\Project Final Machine Learning\\\\spam_chat_detector_using_ML\\\\.venv\\\\nltk_data'\n    - 'd:\\\\Project Final Machine Learning\\\\spam_chat_detector_using_ML\\\\.venv\\\\share\\\\nltk_data'\n    - 'd:\\\\Project Final Machine Learning\\\\spam_chat_detector_using_ML\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Miftah\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to all texts\n",
    "print(\"Preprocessing semua teks...\")\n",
    "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "print(\"\\n✅ Preprocessing selesai!\")\n",
    "print(\"\\nContoh hasil preprocessing:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "    print(f\"Original: {df['text'].iloc[i]}\")\n",
    "    print(f\"Cleaned: {df['cleaned_text'].iloc[i]}\")\n",
    "    print(f\"Label: {df['label'].iloc[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b3c8a3",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction dengan TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ef882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X = df['cleaned_text']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Test set size:\", X_test.shape[0])\n",
    "print(\"\\nLabel distribution in training set:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\nLabel distribution in test set:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fb2e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=3000, ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(\"TF-IDF features shape (train):\", X_train_tfidf.shape)\n",
    "print(\"TF-IDF features shape (test):\", X_test_tfidf.shape)\n",
    "print(f\"\\n✅ TF-IDF vectorization selesai dengan {X_train_tfidf.shape[1]} features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee207fd",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feccb832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary untuk menyimpan model dan hasil evaluasi\n",
    "models = {}\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9099a86b",
   "metadata": {},
   "source": [
    "### 5.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cced5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = lr_model.predict(X_test_tfidf)\n",
    "\n",
    "# Store model\n",
    "models['Logistic Regression'] = lr_model\n",
    "\n",
    "print(\"✅ Logistic Regression training selesai!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745c70d5",
   "metadata": {},
   "source": [
    "### 5.2 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc42dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Bayes\n",
    "print(\"Training Naive Bayes...\")\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "# Store model\n",
    "models['Naive Bayes'] = nb_model\n",
    "\n",
    "print(\"✅ Naive Bayes training selesai!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f028ca2e",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0815700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"\n",
    "    Fungsi untuk evaluasi model\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, pos_label='spam')\n",
    "    recall = recall_score(y_true, y_pred, pos_label='spam')\n",
    "    f1 = f1_score(y_true, y_pred, pos_label='spam')\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"📊 {model_name} - Evaluation Results\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "    print(f\"Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
    "    print(f\"F1-Score:  {f1:.4f} ({f1*100:.2f}%)\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2d4195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Logistic Regression\n",
    "evaluate_model(y_test, y_pred_lr, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555e6f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Naive Bayes\n",
    "evaluate_model(y_test, y_pred_nb, 'Naive Bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa54050",
   "metadata": {},
   "source": [
    "## 7. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de27049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame(results).T\n",
    "print(\"\\n📈 Model Comparison:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Select best model based on F1-Score\n",
    "best_model_name = comparison_df['F1-Score'].idxmax()\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"\\n🏆 Best Model: {best_model_name}\")\n",
    "print(f\"F1-Score: {comparison_df.loc[best_model_name, 'F1-Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49575101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "comparison_df.plot(kind='bar', ax=ax, color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12'])\n",
    "plt.title('Perbandingan Performa Model', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.ylim(0, 1.1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.3f', padding=3, fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04881f6",
   "metadata": {},
   "source": [
    "## 8. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb0037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, model_name):\n",
    "    \"\"\"\n",
    "    Fungsi untuk visualisasi confusion matrix\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=['ham', 'spam'])\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Ham', 'Spam'], \n",
    "                yticklabels=['Ham', 'Spam'],\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.ylabel('Actual', fontsize=12)\n",
    "    plt.xlabel('Predicted', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5d8eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for Logistic Regression\n",
    "cm_lr = plot_confusion_matrix(y_test, y_pred_lr, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ae2c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for Naive Bayes\n",
    "cm_nb = plot_confusion_matrix(y_test, y_pred_nb, 'Naive Bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300f64ae",
   "metadata": {},
   "source": [
    "## 9. Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bd0ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report for best model\n",
    "if best_model_name == 'Logistic Regression':\n",
    "    y_pred_best = y_pred_lr\n",
    "else:\n",
    "    y_pred_best = y_pred_nb\n",
    "\n",
    "print(f\"\\n📋 Classification Report - {best_model_name}:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=['Ham', 'Spam']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7c0821",
   "metadata": {},
   "source": [
    "## 10. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e41d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "with open('../model/model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "print(f\"✅ Model saved: ../model/model.pkl ({best_model_name})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac3e536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save TF-IDF vectorizer\n",
    "with open('../model/tfidf.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "\n",
    "print(\"✅ TF-IDF vectorizer saved: ../model/tfidf.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab075b4",
   "metadata": {},
   "source": [
    "## 11. Test Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb7d903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_spam(text, model, vectorizer):\n",
    "    \"\"\"\n",
    "    Fungsi untuk prediksi spam\n",
    "    \"\"\"\n",
    "    # Preprocess\n",
    "    cleaned = preprocess_text(text)\n",
    "    \n",
    "    # Vectorize\n",
    "    vectorized = vectorizer.transform([cleaned])\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(vectorized)[0]\n",
    "    probability = model.predict_proba(vectorized)[0]\n",
    "    \n",
    "    return prediction, probability\n",
    "\n",
    "# Test predictions\n",
    "test_messages = [\n",
    "    \"Congratulations! You won $1000. Click here now!\",\n",
    "    \"Hey, want to grab lunch tomorrow?\",\n",
    "    \"URGENT: Your account needs verification immediately!\",\n",
    "    \"Can you send me the meeting notes?\"\n",
    "]\n",
    "\n",
    "print(\"\\n🧪 Testing predictions:\\n\")\n",
    "for msg in test_messages:\n",
    "    pred, prob = predict_spam(msg, best_model, tfidf)\n",
    "    spam_prob = prob[1] if pred == 'spam' else prob[0]\n",
    "    print(f\"Message: {msg}\")\n",
    "    print(f\"Prediction: {pred.upper()} (confidence: {spam_prob*100:.2f}%)\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bb882d",
   "metadata": {},
   "source": [
    "## 12. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb6d487",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n📊 Dataset: {df.shape[0]} samples\")\n",
    "print(f\"📚 Training samples: {X_train.shape[0]}\")\n",
    "print(f\"🧪 Test samples: {X_test.shape[0]}\")\n",
    "print(f\"\\n🏆 Best Model: {best_model_name}\")\n",
    "print(f\"📈 Accuracy: {comparison_df.loc[best_model_name, 'Accuracy']:.4f}\")\n",
    "print(f\"📈 F1-Score: {comparison_df.loc[best_model_name, 'F1-Score']:.4f}\")\n",
    "print(f\"\\n💾 Saved files:\")\n",
    "print(f\"   - ../model/model.pkl\")\n",
    "print(f\"   - ../model/tfidf.pkl\")\n",
    "print(\"\\n✨ Model is ready to use in the Streamlit app!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
